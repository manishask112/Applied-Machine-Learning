{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_P556_F19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tg9fRSjFtsRu"
      },
      "source": [
        "# Assignment #3\n",
        "## P556: Applied Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jjsZpx5C9eBH"
      },
      "source": [
        "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. In this assignment you will implement a fully-connected and a convolutional neural network from scratch. To simplify the implementation, we are asking you to implement static architectures, but you are free to support variable number of layers/neurons/activations/optimizers/etc. We recommend that you make use of private methods so you can easily troubleshoot small parts of your model as you develop them, instead of trying to figure out which parts are not working correctly after implementing everything. Also, keep in mind that there is code from your fully-connected neural network that can be re-used on the CNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2NzW9M-btzqO"
      },
      "source": [
        "Problem #1.1 (40 points): Implement a fully-connected neural network from scratch. The neural network will have the following architecture:\n",
        "\n",
        "- Input layer\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Output layer, using softmax as the activation function\n",
        "\n",
        "The model will use categorical crossentropy as its loss function. \n",
        "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
        "We will evaluate the model using accuracy.\n",
        "\n",
        "Why this architecture? We are trying to reproduce from scratch the following [example from the Keras documentation](https://keras.io/examples/mnist_mlp/). This means that you can compare your results by running the Keras code provided above to see if you are on the right track."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFMHckmxfjZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference for Working of Neural Network: Week 4&5 of Andrew Ng's 'Machine Learning' course on Coursera\n",
        "# https://www.coursera.org/learn/machine-learning/home/welcome\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "  def __init__(self,epochs,batch_size,learning_rate):\n",
        "    self.epochs=epochs\n",
        "    self.learning_rate=learning_rate\n",
        "    self.batch_size=batch_size\n",
        "  \n",
        "  def fit(self,X_train,Y_train,neurons_each_layer,rho):\n",
        "    # weights initialization\n",
        "    # Reference for normalizing weights: https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
        "    wi=np.random.randn(neurons_each_layer[1],neurons_each_layer[0])*0.01\n",
        "    wh1=np.random.randn(neurons_each_layer[2],neurons_each_layer[1])*0.01\n",
        "    wh2=np.random.randn(neurons_each_layer[3],neurons_each_layer[2])*0.01\n",
        "\n",
        "    # bias initialization\n",
        "    bias_i=np.zeros((neurons_each_layer[1], 1))\n",
        "    bias_h1=np.zeros((neurons_each_layer[2], 1))\n",
        "    bias_h2=np.zeros((neurons_each_layer[3], 1))\n",
        "\n",
        "    # RMSProp initialization\n",
        "    vdwi,vdbi,vdwh1,vdbh1,vdwh2,vdbh2=float(0),float(0),float(0),float(0),float(0),float(0)\n",
        "\n",
        "    # number_of_batches=int(X_train.shape[0]/self.batch_size)\n",
        "\n",
        "    # Running Dataset through Epochs\n",
        "    # Reference: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
        "    print(\"Running 20 epochs...\")\n",
        "    for j in range(0,self.epochs):\n",
        "      # print(\"epoch \"+str(j))\n",
        "      count=0  \n",
        "      i=0\n",
        "      # Shuffling samples in dataset\n",
        "      # Reference: https://adventuresinmachinelearning.com/stochastic-gradient-descent/\n",
        "      shuffled_indeces = np.random.permutation(X_train.shape[0])\n",
        "      X_train=X_train[shuffled_indeces]\n",
        "      Y_train=Y_train[shuffled_indeces] \n",
        "\n",
        "      # Iterations through batches begins \n",
        "      while(i<X_train.shape[0]):\n",
        "        X = X_train[i:i+self.batch_size]\n",
        "        Y = Y_train[i:i+self.batch_size]\n",
        "        i=i+self.batch_size\n",
        "        count=count+1\n",
        "\n",
        "        # Forward Propogation with Dropout\n",
        "        (A2,D2,A3,D3,Y_pred)=self.forward_prop_with_dropout(X,[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "\n",
        "        # Back Propogation with Dropout\n",
        "        n=X.shape[0]\n",
        "        (dwh2,dbias_h2,dwh1,dbias_h1,dwi,dbias_i)=self.back_prop_with_dropout(n,X,Y,[A2,D2,A3,D3,Y_pred],[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "    \n",
        "        # Gradient Descent with RMSProp\n",
        "        parameters1=[vdwi,vdbi,vdwh1,vdbh1,vdwh2,vdbh2]\n",
        "        parameters2=[dwh2,dbias_h2,dwh1,dbias_h1,dwi,dbias_i]\n",
        "        parameters3=[wi,bias_i,wh1,bias_h1,wh2,bias_h2]\n",
        "        (vdwi,vdbi,vdwh1,vdbh1,vdwh2,vdbh2,wi,bias_i,wh1,bias_h1,wh2,bias_h2)=self.gradient_descent_with_RMSProp(parameters1,parameters2,parameters3,rho)\n",
        "\n",
        "    return (wi,bias_i,wh1,bias_h1,wh2,bias_h2)\n",
        "\n",
        "\n",
        "  # Forward Propogation with Dropout  \n",
        "  # Reference for forward prop: https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
        "  # Reference for dropout: https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
        "  def forward_prop_with_dropout(self,X,parameters):\n",
        "    [wi,bias_i,wh1,bias_h1,wh2,bias_h2]=parameters\n",
        "    \n",
        "    A2=self.relu(np.transpose(X),wi,bias_i)\n",
        "    D2,A2=self.AwithDropout(A2) \n",
        "\n",
        "    A3=self.relu(A2,wh1,bias_h1)\n",
        "    D3,A3=self.AwithDropout(A3)\n",
        "\n",
        "    Y_pred=self.softmax(A3,wh2,bias_h2)\n",
        "\n",
        "    return (A2,D2,A3,D3,Y_pred)\n",
        "\n",
        "  # Backward Propogation with Dropout \n",
        "  # References:\n",
        "  # Formula for Gradient values: https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
        "  # Flow/Chain Rule of Backprop: https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
        "  def back_prop_with_dropout(self,n,X,Y,parameters1,parameters2):\n",
        "    [A2,D2,A3,D3,Y_pred]=parameters1\n",
        "    [wi,bias_i,wh1,bias_h1,wh2,bias_h2]=parameters2\n",
        "\n",
        "    # dY_pred=Y_pred-np.transpose(Y)\n",
        "    # del3=np.multiply(dY_pred,self.softmax_backpropogation(Y_pred))\n",
        "\n",
        "    # Derivative wrt to loss is just Y'-Y\n",
        "    # Reference: https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
        "    del3=Y_pred-np.transpose(Y)\n",
        "    dwh2=float(1)/float(n)*(np.dot(del3,np.transpose(A3)))\n",
        "    dbias_h2=float(1)/float(n)*np.sum(del3,axis=1,keepdims=True)\n",
        "    dA3=np.dot(np.transpose(wh2),del3)\n",
        "    dA3=(dA3*D3)/0.8\n",
        "\n",
        "    del2=np.multiply(dA3,self.relu_backprop(A3))\n",
        "    dwh1=float(1)/float(n)*(np.dot(del2,np.transpose(A2)))\n",
        "    dbias_h1=float(1)/float(n)*np.sum(del2,axis=1,keepdims=True)\n",
        "    dA2=np.dot(np.transpose(wh1),del2)\n",
        "    dA2=(dA2*D2)/0.8\n",
        "    \n",
        "    del1=np.multiply(dA2,self.relu_backprop(A2))\n",
        "    dwi=float(1)/float(n)*(np.dot(del1,X))\n",
        "    dbias_i=float(1)/float(n)*np.sum(del1,axis=1,keepdims=True)\n",
        "\n",
        "    return (dwh2,dbias_h2,dwh1,dbias_h1,dwi,dbias_i)\n",
        "\n",
        "  # Gradient Descent with RMSProp Optimizer\n",
        "  # References:\n",
        "  # Implementing RMSProp: https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b\n",
        "  # Changing value of learning rate with RMSProp: https://towardsdatascience.com/10-gradient-descent-optimisation-algorithms-86989510b5e9\n",
        "  def gradient_descent_with_RMSProp(self,parameters1,parameters2,parameters3,rho):\n",
        "    [vdwi,vdbi,vdwh1,vdbh1,vdwh2,vdbh2]=parameters1\n",
        "    [dwh2,dbias_h2,dwh1,dbias_h1,dwi,dbias_i]=parameters2\n",
        "    [wi,bias_i,wh1,bias_h1,wh2,bias_h2]=parameters3\n",
        "\n",
        "    vdwi=(rho*vdwi) +((1-rho)*(dwi*dwi))\n",
        "    vdbi=(rho*vdbi)+((1-rho)*(dbias_i*dbias_i))\n",
        "    new_learning_rate_w=self.learning_rate/(np.sqrt(vdwi+0.00001))\n",
        "    wi=wi-(new_learning_rate_w*dwi)\n",
        "    new_learning_rate_b=self.learning_rate/(np.sqrt(vdbi+0.00001))\n",
        "    bias_i=bias_i-(new_learning_rate_b*dbias_i)\n",
        "\n",
        "    vdwh1=(rho*vdwh1)+((1-rho)*(dwh1*dwh1))\n",
        "    vdbh1=(rho*vdbh1)+((1-rho)*(dbias_h1*dbias_h1))\n",
        "    new_learning_rate_w=self.learning_rate/(np.sqrt(vdwh1+0.00001))\n",
        "    wh1=wh1-(new_learning_rate_w*dwh1)\n",
        "    new_learning_rate_b=self.learning_rate/(np.sqrt(vdbh1+0.00001))\n",
        "    bias_h1=bias_h1-(new_learning_rate_b*dbias_h1)\n",
        "\n",
        "    vdwh2=(rho*vdwh2)+((1-rho)*(dwh2*dwh2))\n",
        "    vdbh2=(rho*vdbh2)+((1-rho)*(dbias_h2*dbias_h2))\n",
        "    new_learning_rate_w=self.learning_rate/(np.sqrt(vdwh2+0.00001))\n",
        "    wh2=wh2-(new_learning_rate_w*dwh2)\n",
        "    new_learning_rate_b=self.learning_rate/(np.sqrt(vdbh2+0.00001))\n",
        "    bias_h2=bias_h2-(new_learning_rate_b*dbias_h2)\n",
        "\n",
        "    return (vdwi,vdbi,vdwh1,vdbh1,vdwh2,vdbh2,wi,bias_i,wh1,bias_h1,wh2,bias_h2)\n",
        "\n",
        "  # References for ReLU and derivative of ReLU:\n",
        "  # https://stats.stackexchange.com/questions/333394/what-is-the-derivative-of-the-relu-activation-function\n",
        "  # https://www.coursera.org/lecture/neural-networks-deep-learning/derivatives-of-activation-functions-qcG1j\n",
        "  def relu(self,A,W,BIAS):\n",
        "    Z=np.dot(W,A)+BIAS\n",
        "    # print(\"Z_relu \"+str(Z.shape))\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "  def relu_backprop(self,A):\n",
        "    return float(1)*(A>0)     \n",
        "  \n",
        "  # Softmax Calculation\n",
        "  # Reference: https://stackoverflow.com/questions/54880369/implementation-of-softmax-function-returns-nan-for-high-inputs\n",
        "  # https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
        "  def softmax(self,A,W,BIAS):\n",
        "    Z=np.dot(W,A)+BIAS\n",
        "    # print(\"Z_softmax \"+str(Z.shape))\n",
        "    E=np.exp(Z-np.max(Z))\n",
        "    return E/np.sum(E, axis=0)\n",
        "\n",
        "  # Dropout Optimization \n",
        "  # Reference for creating a dropout matrix:\n",
        "  # https://gluon.mxnet.io/chapter03_deep-neural-networks/mlp-dropout-scratch.html\n",
        "  # https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
        "  def AwithDropout(self,A):\n",
        "    Dropout_matrix=np.random.rand(A.shape[0], A.shape[1])<0.8\n",
        "    # print(Dropout_matrix)\n",
        "    return Dropout_matrix,(A*Dropout_matrix)/0.8\n",
        "\n",
        "  # This function calls Forward Prop and returns accuracy and loss for the prediction as is returned by the evaluate function in Keras Documentation\n",
        "  def evaluate(self,X,Y,parameters):\n",
        "    [wi,bias_i,wh1,bias_h1,wh2,bias_h2]=parameters\n",
        "\n",
        "    # Running one Forward Prop\n",
        "    Y_pred=self.forward_prop_with_dropout(X,parameters)[4]\n",
        "\n",
        "    # Calculating Accuracy for Prediction(Number of correct predictions/Number of samples*100)\n",
        "    Y_pred=np.transpose(Y_pred)\n",
        "    count=0\n",
        "    for i in range(0,len(Y_pred)):\n",
        "      # np.argmax reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n",
        "      if np.argmax(Y_pred[i])==np.argmax(Y[i]):\n",
        "        count=count+1\n",
        "    # print(\"count \"+str(count))   \n",
        "    accuracy=float(count)/float(Y.shape[0])*100\n",
        "\n",
        "    # Calculating Loss incurred\n",
        "    # Reference: https://medium.com/towards-artificial-intelligence/logistic-regression-in-python-from-scratch-954c0196d258\n",
        "    # https://medium.com/data-science-bootcamp/understand-cross-entropy-loss-in-minutes-9fb263caee9a\n",
        "    Y_pred=np.transpose(Y_pred)\n",
        "    Y=np.transpose(Y)\n",
        "    # After using cross entropy for binary classification, I got the error \"divide by zero encountered in log\" \n",
        "    # Then from the above link, I referred the correct implementation of cross-entropy for multivariate classification\n",
        "    cost_function=(-float(1)/float(Y.shape[1]))*np.sum(np.multiply(Y,np.log(Y_pred)))\n",
        "    # accuracy=(cost_function/float(Y_TRAIN.shape[1]))*100\n",
        "    return accuracy,cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DH3bgJyPuE2O"
      },
      "source": [
        "Problem #1.2 (10 points): Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsN4sUoUugl8",
        "outputId": "5cae7e41-a86a-4d69-e882-5a196a46a838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
        "# library. Keras can be installed using pip: python -m pip install keras\n",
        "\n",
        "# Original source for the dataset:\n",
        "# https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "# Reference to the Fashion-MNIST's Keras function: \n",
        "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
        "\n",
        "# Concepts and approaches have been discussed with Deepthi Raghu(draghu)\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "import keras.utils\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "# print(x_train.shape)\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "N=NeuralNetwork(20,128,0.001)\n",
        "# (wi,bias_i,wh1,bias_h1,wh2,bias_h2)=N.fit(x_train,y_train,[784,512,512,10],0.9)\n",
        "# accuracy=N.evaluate(x_train,y_train,[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "# print(accuracy)\n",
        "\n",
        "# 5-Fold Cross Validation\n",
        "# Reference:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "KF=KFold(n_splits=5)\n",
        "count=0\n",
        "for i,j in KF.split(x_train):\n",
        "  count=count+1\n",
        "  X_TRAIN=x_train[i]\n",
        "  X_VALIDATION=x_train[j]\n",
        "  Y_TRAIN=y_train[i]\n",
        "  Y_VALIDATION=y_train[j]\n",
        "  print('\\n')\n",
        "  print('\\n')\n",
        "  print(\"Fit for fold \"+str(count))\n",
        "  (wi,bias_i,wh1,bias_h1,wh2,bias_h2)=N.fit(X_TRAIN,Y_TRAIN,[784,512,512,10],0.9)\n",
        "  accuracy,cost_function=N.evaluate(X_TRAIN,Y_TRAIN,[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "  print('\\n')\n",
        "  print(\"Loss for Train Set \"+str(cost_function))\n",
        "  print(\"Accuracy for Train Set \"+str(accuracy))\n",
        "  print('\\n')\n",
        "  accuracy,cost_function=N.evaluate(X_VALIDATION,Y_VALIDATION,[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "  print(\"Loss for Validation Set \"+str(cost_function))\n",
        "  print(\"Accuracy for Validation Set \"+str(accuracy))\n",
        "  print('\\n')\n",
        "  accuracy,cost_function=N.evaluate(x_test,y_test,[wi,bias_i,wh1,bias_h1,wh2,bias_h2])\n",
        "  print(\"Loss for Test Set \"+str(cost_function))\n",
        "  print(\"Accuracy for Test Set \"+str(accuracy))\n",
        "  print('\\n')\n",
        "  print('\\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fit for fold 1\n",
            "Running 20 epochs...\n",
            "\n",
            "\n",
            "Loss for Train Set 0.2527936119598102\n",
            "Accuracy for Train Set 90.46041666666666\n",
            "\n",
            "\n",
            "Loss for Validation Set 0.33706246278769797\n",
            "Accuracy for Validation Set 87.73333333333333\n",
            "\n",
            "\n",
            "Loss for Test Set 0.36287380329087143\n",
            "Accuracy for Test Set 87.1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fit for fold 2\n",
            "Running 20 epochs...\n",
            "\n",
            "\n",
            "Loss for Train Set 0.22756219574228012\n",
            "Accuracy for Train Set 91.53333333333333\n",
            "\n",
            "\n",
            "Loss for Validation Set 0.3244919396184544\n",
            "Accuracy for Validation Set 88.63333333333333\n",
            "\n",
            "\n",
            "Loss for Test Set 0.3572972424286358\n",
            "Accuracy for Test Set 87.74\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fit for fold 3\n",
            "Running 20 epochs...\n",
            "\n",
            "\n",
            "Loss for Train Set 0.22200405528878206\n",
            "Accuracy for Train Set 91.62916666666666\n",
            "\n",
            "\n",
            "Loss for Validation Set 0.30218567727123735\n",
            "Accuracy for Validation Set 89.3\n",
            "\n",
            "\n",
            "Loss for Test Set 0.33708962821009386\n",
            "Accuracy for Test Set 88.14999999999999\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fit for fold 4\n",
            "Running 20 epochs...\n",
            "\n",
            "\n",
            "Loss for Train Set 0.22088261353095642\n",
            "Accuracy for Train Set 91.78333333333333\n",
            "\n",
            "\n",
            "Loss for Validation Set 0.3068705208331343\n",
            "Accuracy for Validation Set 89.28333333333333\n",
            "\n",
            "\n",
            "Loss for Test Set 0.34509554528311553\n",
            "Accuracy for Test Set 88.39\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fit for fold 5\n",
            "Running 20 epochs...\n",
            "\n",
            "\n",
            "Loss for Train Set 0.2296000475589049\n",
            "Accuracy for Train Set 91.21666666666667\n",
            "\n",
            "\n",
            "Loss for Validation Set 0.3255819438947751\n",
            "Accuracy for Validation Set 88.725\n",
            "\n",
            "\n",
            "Loss for Test Set 0.3579396817793975\n",
            "Accuracy for Test Set 87.62\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NsYWLhVUt_In"
      },
      "source": [
        "Problem #2.1 (40 points): Implement a Convolutional Neural Network from scratch. Similarly to problem 1.1, we will be implementing the same architecture as the one shown in [Keras' CNN documentation](https://keras.io/examples/mnist_cnn/). That is:\n",
        "\n",
        "- Input layer\n",
        "- Convolutional hidden layer with 32 neurons, a kernel size of (3,3), and relu activation function\n",
        "- Convolutional hidden layer with 64 neurons, a kernel size of (3,3), and relu activation function\n",
        "- Maxpooling with a pool size of (2,2)\n",
        "- Dropout with a value of 0.25\n",
        "- Flatten layer\n",
        "- Dense hidden layer, with 128 neurons, and relu activation function\n",
        "- Dropout with a value of 0.5\n",
        "- Output layer, using softmax as the activation function\n",
        "\n",
        "Our loss function is categorical crossentropy and the evaluation will be done using accuracy, as in Problem 1.1. However, we will not be using the gradient optimizer known as Adadelta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wEGyV8qTvX7H",
        "colab": {}
      },
      "source": [
        "class ConvolutionalNeuralNetwork(object):\n",
        "  def __init__(epochs, learning_rate):\n",
        "    pass\n",
        "  \n",
        "  def fit(self):\n",
        "    pass\n",
        "  \n",
        "  def evaluate(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nmek2iUovYEk"
      },
      "source": [
        "Problem #2.2 (10 points): Train your convolutional neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B6VsoxNSwFVH",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}